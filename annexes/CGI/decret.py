import json
import qdrant_client
from qdrant_client.models import PointStruct, VectorParams, Distance
import voyageai
import uuid
import re
import logging
from datetime import datetime
from tqdm import tqdm
import os

# Configuration
VOYAGE_API_KEY = "pa-gPu9JZffTtb0O57mU8ZNtCzWBrQ7dDRy_7M_f6Cr8br"
voyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)

QDRANT_HOST = "13.39.82.37"
QDRANT_PORT = 6333
EMBEDDING_DIM = 1024

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DecretProcessor:
    """Processeur pour l'embedding des d√©crets avec enrichissement contextuel"""
    
    def __init__(self):
        self.qdrant_client = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        self.collection_name = "cgi_annexe_optimized"  # M√™me collection que annnexevoyagev2
        self.points = []
        
    def process_decret_document(self, doc, doc_index):
        """Traite un document d√©cret avec enrichissement contextuel"""
        doc_id = str(uuid.uuid4())
        
        # Extraire les infos sp√©cifiques aux d√©crets
        document = doc.get("document", "")
        article = doc.get("article", "")
        contenu = doc.get("contenu", "")
        
        if not contenu or not contenu.strip():
            logger.warning(f"Document {doc_index} vide, ignor√©")
            return None
        
        # Extraire les informations enrichies du d√©cret
        doc_info = self._extract_enhanced_info(document, article, contenu)
        
        # ENRICHISSEMENT CONTEXTUEL SP√âCIFIQUE AUX D√âCRETS
        enriched_text_parts = []
        
        # 1. M√©tadonn√©es structur√©es
        enriched_text_parts.append(f"Type de document: {doc_info['type']}")
        enriched_text_parts.append(f"Num√©ro: {doc_info['numero']}")
        enriched_text_parts.append(f"Date: {doc_info['date']}")
        enriched_text_parts.append(f"Article: {article}")
        enriched_text_parts.append(f"Objet: {doc_info['objet']}")
        
        # 2. Articles CGI li√©s (extraction automatique)
        articles_cgi = self._extract_articles_cgi(contenu)
        if articles_cgi:
            enriched_text_parts.append(f"Articles CGI li√©s: {', '.join(f'article {a}' for a in articles_cgi)}")
        
        # 3. Extraction et enrichissement des concepts cl√©s
        key_concepts = self._extract_key_concepts(contenu)
        if key_concepts:
            enriched_text_parts.append(f"Concepts cl√©s: {', '.join(key_concepts)}")
        
        # 4. Enrichissement sp√©cifique par sujet
        sujets = self._extract_sujets(document, contenu)
        if sujets:
            enriched_text_parts.append(f"Sujets: {', '.join(sujets)}")
        
        # 5. Enrichissement contextuel pour les d√©crets sp√©cifiques
        if "fusion" in document.lower() and "stock" in document.lower():
            enriched_text_parts.append("Contexte: fusion-absorption, √©valuation des stocks, soci√©t√© absorb√©e, soci√©t√© absorbante")
            enriched_text_parts.append("Proc√©dure: modalit√©s d'√©valuation, prix de revient initial, valeur d'origine")
            
        if "b√©n√©fice forfaitaire" in document.lower():
            enriched_text_parts.append("Contexte: r√©gime fiscal, b√©n√©fice forfaitaire, professions exclues")
            enriched_text_parts.append("Professions: lib√©rales, commerciales, artisanales, exclusions du forfait")
        
        # 6. Contenu principal
        enriched_text_parts.append(f"Contenu complet: {contenu}")
        
        # Cr√©er le texte enrichi final
        enriched_text = "\n\n".join(enriched_text_parts)
        
        # G√©n√©rer l'embedding
        try:
            result = voyage_client.embed(
                [enriched_text], 
                model="voyage-law-2",
                input_type="document"
            )
            embedding = result.embeddings[0]
        except Exception as e:
            logger.error(f"Erreur embedding doc {doc_index}: {e}")
            return None
        
        # Extraire des m√©tadonn√©es suppl√©mentaires pour la recherche
        search_keywords = self._generate_search_keywords(document, contenu, doc_info['type'])
        
        # Cr√©er le point Qdrant
        point = PointStruct(
            id=doc_id,
            vector=embedding,
            payload={
                # Infos de base sp√©cifiques aux d√©crets
                "type": doc_info["type"],
                "numero": doc_info["numero"],
                "date": doc_info["date"],
                "document": document,
                "article": article,
                "contenu": contenu,
                "objet": doc_info["objet"],
                
                # Articles CGI li√©s
                "articles_lies": articles_cgi,
                
                # M√©tadonn√©es enrichies
                "search_keywords": search_keywords,
                "key_concepts": key_concepts,
                "sujets": sujets,
                "enriched_text": enriched_text,
                
                # Flags sp√©cifiques aux d√©crets
                "has_fusion": "fusion" in document.lower() or "fusion" in contenu.lower(),
                "has_stock": "stock" in document.lower() or "stock" in contenu.lower(),
                "has_benefice_forfaitaire": "b√©n√©fice forfaitaire" in document.lower() or "forfaitaire" in contenu.lower(),
                "has_professions_exclues": "professions" in document.lower() and "exclues" in document.lower(),
                "has_evaluation": "√©valuation" in contenu.lower(),
                "has_modalites": "modalit√©s" in contenu.lower(),
                
                # M√©tadonn√©es techniques
                "doc_index": doc_index,
                "source_file": "annexes/CGI/JSON/decret.json",
                "indexed_at": datetime.now().isoformat()
            }
        )
        
        return point
    
    def _extract_enhanced_info(self, document, article, contenu):
        """Extraction am√©lior√©e des informations du d√©cret"""
        doc_info = {
            "type": "decret",
            "numero": "",
            "date": "",
            "objet": ""
        }
        
        # Extraire le num√©ro du d√©cret
        numero_match = re.search(r'n¬∞\s*(\d+-\d+-\d+)', document)
        if numero_match:
            doc_info["numero"] = numero_match.group(1)
        
        # Extraire la date
        date_match = re.search(r'\((\d+\s+\w+\s+\d+)\)', document)
        if date_match:
            doc_info["date"] = date_match.group(1)
        
        # Extraire l'objet (apr√®s "relatif")
        objet_match = re.search(r'relatif\s+(.+?)(?:\.|$)', document, re.IGNORECASE)
        if objet_match:
            doc_info["objet"] = objet_match.group(1).strip()
        
        return doc_info
    
    def _extract_articles_cgi(self, contenu):
        """Extrait les r√©f√©rences aux articles du CGI"""
        articles = []
        
        # Patterns pour d√©tecter les articles CGI
        patterns = [
            r'article\s+(\d+(?:-[IVX]+)?)',
            r'l\'article\s+(\d+(?:-[IVX]+)?)',
            r'articles?\s+(\d+(?:-[IVX]+)?(?:\s+et\s+\d+(?:-[IVX]+)?)*)',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, contenu, re.IGNORECASE)
            for match in matches:
                if 'et' in match:
                    # G√©rer les cas "article 20 et 162"
                    nums = re.findall(r'\d+(?:-[IVX]+)?', match)
                    articles.extend(nums)
                else:
                    articles.append(match)
        
        return list(set(articles))[:10]  # Maximum 10 articles
    
    def _extract_key_concepts(self, contenu):
        """Extrait les concepts cl√©s du contenu"""
        concepts = []
        contenu_lower = contenu.lower()
        
        # Concepts sp√©cifiques aux d√©crets
        concept_patterns = {
            "fusion-absorption": r"fusion|absorption|soci√©t√©\s+absorb√©e|soci√©t√©\s+absorbante",
            "√©valuation stocks": r"√©valuation.*stock|modalit√©s.*√©valuation",
            "prix de revient": r"prix\s+de\s+revient|valeur\s+d'origine",
            "b√©n√©fice forfaitaire": r"b√©n√©fice\s+forfaitaire|r√©gime\s+forfaitaire",
            "professions exclues": r"professions.*exclues|activit√©s.*exclues",
            "√©tat d√©taill√©": r"√©tat\s+d√©taill√©|√©tat\s+de\s+suivi",
            "convention fusion": r"convention\s+de\s+fusion",
            "d√©claration r√©sultat": r"d√©claration.*r√©sultat\s+fiscal",
            "prix du march√©": r"prix\s+du\s+march√©",
            "note explicative": r"note\s+explicative"
        }
        
        for concept, pattern in concept_patterns.items():
            if re.search(pattern, contenu_lower):
                concepts.append(concept)
        
        # Extraire les professions mentionn√©es
        professions = [
            "architectes", "avocats", "m√©decins", "pharmaciens", "notaires",
            "experts-comptables", "ing√©nieurs", "v√©t√©rinaires", "dentistes",
            "h√¥teliers", "promoteurs immobiliers", "agents d'affaires"
        ]
        
        for profession in professions:
            if profession in contenu_lower:
                concepts.append(profession)
        
        return list(set(concepts))[:15]
    
    def _extract_sujets(self, document, contenu):
        """Extrait les sujets principaux du d√©cret"""
        sujets = []
        
        # Sujets bas√©s sur le titre du document
        if "fusion" in document.lower():
            sujets.extend(["fusion-absorption", "restructuration", "√©valuation patrimoniale"])
        
        if "forfaitaire" in document.lower():
            sujets.extend(["r√©gime fiscal", "professions lib√©rales", "exclusions fiscales"])
        
        # Sujets bas√©s sur le contenu
        if "stock" in contenu.lower():
            sujets.append("gestion des stocks")
        
        if "convention" in contenu.lower():
            sujets.append("proc√©dures administratives")
        
        return list(set(sujets))
    
    def _generate_search_keywords(self, document, contenu, doc_type):
        """G√©n√®re des mots-cl√©s optimis√©s pour la recherche"""
        keywords = []
        
        # Mots du titre du document
        doc_words = [w for w in document.lower().split() if len(w) > 3]
        keywords.extend(doc_words[:10])
        
        # Type de document
        keywords.append(doc_type)
        keywords.append("d√©cret")
        
        # Termes fiscaux et juridiques importants
        fiscal_terms = [
            "fusion", "absorption", "stock", "√©valuation", "forfaitaire",
            "professions", "exclues", "modalit√©s", "convention", "soci√©t√©",
            "prix", "revient", "march√©", "√©tat", "d√©taill√©", "suivi",
            "d√©claration", "r√©sultat", "fiscal", "bulletin", "officiel"
        ]
        
        for term in fiscal_terms:
            if term in contenu.lower() or term in document.lower():
                keywords.append(term)
        
        return list(set(keywords))[:25]
    
    def process_all_documents(self, documents):
        """Traite tous les documents d√©crets"""
        logger.info(f"Traitement de {len(documents)} documents d√©crets...")
        
        # V√©rifier que la collection existe (ne pas la supprimer)
        try:
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            logger.info(f"Collection {self.collection_name} trouv√©e avec {collection_info.points_count} documents existants")
        except Exception as e:
            logger.error(f"Erreur: Collection {self.collection_name} non trouv√©e: {e}")
            logger.info("Cr√©ation de la collection...")
            self.qdrant_client.create_collection(
                collection_name=self.collection_name,
                vectors_config=VectorParams(size=EMBEDDING_DIM, distance=Distance.COSINE)
            )
        
        # Traiter chaque document
        for i, doc in enumerate(tqdm(documents, desc="Traitement des d√©crets")):
            point = self.process_decret_document(doc, i)
            if point:
                self.points.append(point)
        
        # Indexer tous les points
        if self.points:
            logger.info(f"Ajout de {len(self.points)} nouveaux documents √† la collection...")
            
            # Indexer par batch
            batch_size = 20
            for i in range(0, len(self.points), batch_size):
                batch = self.points[i:i+batch_size]
                try:
                    self.qdrant_client.upsert(
                        collection_name=self.collection_name,
                        points=batch
                    )
                    logger.info(f"Batch {i//batch_size + 1} index√©")
                except Exception as e:
                    logger.error(f"Erreur indexation batch: {e}")
            
            # V√©rifier le r√©sultat final
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            logger.info(f"‚úÖ Collection mise √† jour avec {collection_info.points_count} documents au total")
        
        return len(self.points)

def main():
    """Script principal pour indexer les d√©crets"""
    logger.info("üöÄ D√©marrage de l'indexation des d√©crets")
    
    # Charger les donn√©es
    with open("annexes/CGI/JSON/decret.json", "r", encoding="utf-8") as f:
        decret_data = json.load(f)
    
    logger.info(f"‚úÖ {len(decret_data)} documents d√©crets charg√©s")
    
    # Cr√©er le processeur
    processor = DecretProcessor()
    
    # Traiter tous les documents
    count = processor.process_all_documents(decret_data)
    
    logger.info(f"‚úÖ Indexation termin√©e: {count} nouveaux documents d√©crets ajout√©s")
    
    # Sauvegarder les stats
    stats = {
        "total_documents": len(decret_data),
        "indexed_documents": count,
        "collection_name": processor.collection_name,
        "source_file": "annexes/CGI/JSON/decret.json",
        "timestamp": datetime.now().isoformat()
    }
    
    with open("decret_indexing_stats.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    logger.info("‚úÖ Stats sauvegard√©es dans decret_indexing_stats.json")

if __name__ == "__main__":
    main()