import json
import qdrant_client
from qdrant_client.models import PointStruct, VectorParams, Distance
import voyageai
import uuid
import re
import logging
from datetime import datetime
from tqdm import tqdm
import os

# Configuration
VOYAGE_API_KEY = "pa-gPu9JZffTtb0O57mU8ZNtCzWBrQ7dDRy_7M_f6Cr8br"
voyage_client = voyageai.Client(api_key=VOYAGE_API_KEY)

QDRANT_HOST = "13.39.82.37"
QDRANT_PORT = 6333
EMBEDDING_DIM = 1024

# Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class FAQProcessor:
    """Processeur pour l'embedding des FAQ avec enrichissement contextuel"""
    
    def __init__(self):
        self.qdrant_client = qdrant_client.QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT)
        self.collection_name = "cgi_annexe_optimized"  # M√™me collection que annexevoyagev2
        self.points = []
        
    def process_faq_entry(self, entry, entry_index):
        """Traite une entr√©e FAQ avec enrichissement contextuel"""
        entry_id = str(uuid.uuid4())
        
        # Extraire les infos
        document = entry.get("document", "")
        question = entry.get("question", "")
        reponse = entry.get("reponse", "")
        
        if not question or not reponse or not question.strip() or not reponse.strip():
            logger.warning(f"Entr√©e FAQ {entry_index} incompl√®te, ignor√©e")
            return None
        
        # ENRICHISSEMENT CONTEXTUEL SP√âCIFIQUE POUR FAQ
        enriched_text_parts = []
        
        # 1. M√©tadonn√©es structur√©es
        enriched_text_parts.append(f"Type de document: FAQ")
        enriched_text_parts.append(f"Document source: {document}")
        enriched_text_parts.append(f"Question: {question}")
        
        # 2. Extraction des concepts cl√©s de la question et r√©ponse
        key_concepts = self._extract_faq_concepts(question, reponse)
        if key_concepts:
            enriched_text_parts.append(f"Concepts cl√©s: {', '.join(key_concepts)}")
        
        # 3. D√©tection des articles CGI mentionn√©s
        articles_cgi = self._extract_articles_cgi(reponse)
        if articles_cgi:
            enriched_text_parts.append(f"Articles CGI mentionn√©s: {', '.join(articles_cgi)}")
        
        # 4. Enrichissement sp√©cifique APP (Accord Pr√©alable Prix de Transfert)
        if "APP" in document or "prix de transfert" in document.lower():
            enriched_text_parts.append("Sujet: Accord Pr√©alable en mati√®re de Prix de Transfert")
            enriched_text_parts.append("Domaine: prix de transfert, transactions transfrontali√®res, entreprises associ√©es")
            
            # Concepts sp√©cifiques APP
            if "bilat√©ral" in reponse.lower() or "multilat√©ral" in reponse.lower():
                enriched_text_parts.append("Type APP: accord bilat√©ral ou multilat√©ral")
            if "garantie" in reponse.lower():
                enriched_text_parts.append("Aspect: garanties et s√©curit√© juridique")
            if "r√©vision" in reponse.lower() or "annulation" in reponse.lower():
                enriched_text_parts.append("Aspect: r√©vision et annulation d'accord")
            if "contr√¥le" in reponse.lower():
                enriched_text_parts.append("Aspect: contr√¥le fiscal et proc√©dures")
        
        # 5. Contenu principal
        enriched_text_parts.append(f"R√©ponse compl√®te: {reponse}")
        
        # Cr√©er le texte enrichi final
        enriched_text = "\n\n".join(enriched_text_parts)
        
        # G√©n√©rer l'embedding
        try:
            result = voyage_client.embed(
                [enriched_text], 
                model="voyage-law-2",
                input_type="document"
            )
            embedding = result.embeddings[0]
        except Exception as e:
            logger.error(f"Erreur embedding entr√©e FAQ {entry_index}: {e}")
            return None
        
        # Extraire des m√©tadonn√©es suppl√©mentaires pour la recherche
        search_keywords = self._generate_faq_keywords(document, question, reponse)
        
        # Cr√©er le point Qdrant
        point = PointStruct(
            id=entry_id,
            vector=embedding,
            payload={
                # Infos de base FAQ
                "type": "faq",
                "document_source": document,
                "question": question,
                "reponse": reponse,
                "contenu": f"Q: {question}\n\nR: {reponse}",  # Format unifi√©
                
                # Articles CGI d√©tect√©s
                "articles_lies": articles_cgi,
                
                # M√©tadonn√©es enrichies
                "search_keywords": search_keywords,
                "key_concepts": key_concepts,
                "enriched_text": enriched_text,
                
                # Flags sp√©cifiques FAQ APP
                "has_app": "APP" in document or "prix de transfert" in document.lower(),
                "has_bilateral": "bilat√©ral" in reponse.lower() or "multilat√©ral" in reponse.lower(),
                "has_garantie": "garantie" in reponse.lower(),
                "has_controle": "contr√¥le" in reponse.lower(),
                "has_revision": "r√©vision" in reponse.lower() or "annulation" in reponse.lower(),
                "has_expertise": "expert" in reponse.lower(),
                "has_confidentialite": "confidentialit√©" in reponse.lower(),
                
                # M√©tadonn√©es
                "entry_index": entry_index,
                "indexed_at": datetime.now().isoformat()
            }
        )
        
        return point
    
    def _extract_faq_concepts(self, question, reponse):
        """Extrait les concepts cl√©s de la question et r√©ponse FAQ"""
        concepts = []
        text_combined = f"{question} {reponse}".lower()
        
        # Patterns de concepts importants pour APP
        concept_patterns = {
            "accord pr√©alable": r"accord\s+pr√©alable",
            "prix de transfert": r"prix\s+de\s+transfert",
            "entreprises associ√©es": r"entreprises?\s+associ√©es?",
            "transactions transfrontali√®res": r"transactions?\s+transfrontali√®res?",
            "APP bilat√©ral": r"app\s+bilat√©ral",
            "APP multilat√©ral": r"app\s+multilat√©ral",
            "double imposition": r"double\s+(?:ou\s+multiple\s+)?imposition",
            "m√©thode d√©termination": r"m√©thode\s+de\s+d√©termination",
            "contr√¥le fiscal": r"contr√¥le\s+fiscal",
            "proc√©dure rectification": r"proc√©dure\s+de\s+rectification",
            "convention fiscale": r"convention\s+fiscale",
            "administration fiscale": r"administration\s+fiscale",
            "expertise externe": r"expert(?:ise)?\s+externe",
            "confidentialit√©": r"confidentialit√©",
            "4 ans maximum": r"(?:4\s+ans|quatre\s+ans)"
        }
        
        for concept, pattern in concept_patterns.items():
            if re.search(pattern, text_combined):
                concepts.append(concept)
        
        # Extraire les dur√©es
        durations = re.findall(r'\d+\s+(?:ans?|ann√©es?|mois)', text_combined)
        concepts.extend(durations[:2])
        
        return list(set(concepts))[:15]
    
    def _extract_articles_cgi(self, text):
        """Extrait les r√©f√©rences aux articles du CGI"""
        articles = []
        
        # Patterns pour articles CGI
        patterns = [
            r'article\s+(\d+(?:\s+bis|\s+ter)?)',
            r'articles?\s+(\d+(?:\s+bis|\s+ter)?)(?:\s+(?:ou|et)\s+(\d+(?:\s+bis|\s+ter)?))?',
            r'(\d+(?:\s+bis|\s+ter)?)\s+du\s+(?:code\s+g√©n√©ral\s+des\s+imp√¥ts|cgi)'
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, text.lower())
            for match in matches:
                if isinstance(match, tuple):
                    articles.extend([m for m in match if m])
                else:
                    articles.append(match)
        
        # Nettoyer et formater
        cleaned_articles = []
        for article in articles:
            article = article.strip()
            if article and article not in cleaned_articles:
                cleaned_articles.append(f"article {article}")
        
        return cleaned_articles[:5]
    
    def _generate_faq_keywords(self, document, question, reponse):
        """G√©n√®re des mots-cl√©s optimis√©s pour la recherche FAQ"""
        keywords = []
        
        # Mots de la question (les plus importants)
        question_words = [w for w in question.lower().split() if len(w) > 3]
        keywords.extend(question_words[:8])
        
        # Type de document
        keywords.append("faq")
        keywords.append("app")
        
        # Termes fiscaux importants
        fiscal_terms = [
            "accord", "pr√©alable", "prix", "transfert", "app", "bilat√©ral", 
            "multilat√©ral", "garantie", "contr√¥le", "fiscal", "r√©vision", 
            "annulation", "expertise", "confidentialit√©", "entreprise", 
            "associ√©e", "transaction", "transfrontali√®re", "imposition",
            "m√©thode", "d√©termination", "convention", "administration"
        ]
        
        text_combined = f"{question} {reponse}".lower()
        for term in fiscal_terms:
            if term in text_combined:
                keywords.append(term)
        
        return list(set(keywords))[:25]
    
    def process_all_faq_entries(self, faq_data):
        """Traite toutes les entr√©es FAQ"""
        logger.info(f"Traitement de {len(faq_data)} entr√©es FAQ...")
        
        # V√©rifier que la collection existe
        try:
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            logger.info(f"Collection {self.collection_name} trouv√©e avec {collection_info.points_count} documents existants")
        except Exception as e:
            logger.error(f"Collection {self.collection_name} non trouv√©e: {e}")
            raise
        
        # Traiter chaque entr√©e FAQ
        for i, entry in enumerate(tqdm(faq_data, desc="Traitement des FAQ")):
            point = self.process_faq_entry(entry, i)
            if point:
                self.points.append(point)
        
        # Indexer tous les points (AJOUT √† la collection existante)
        if self.points:
            logger.info(f"Ajout de {len(self.points)} entr√©es FAQ √† la collection existante...")
            
            # Indexer par batch
            batch_size = 20
            for i in range(0, len(self.points), batch_size):
                batch = self.points[i:i+batch_size]
                try:
                    self.qdrant_client.upsert(
                        collection_name=self.collection_name,
                        points=batch
                    )
                    logger.info(f"Batch FAQ {i//batch_size + 1} index√©")
                except Exception as e:
                    logger.error(f"Erreur indexation batch FAQ: {e}")
            
            # V√©rifier le r√©sultat final
            collection_info = self.qdrant_client.get_collection(self.collection_name)
            logger.info(f"‚úÖ Collection mise √† jour avec {collection_info.points_count} documents au total")
        
        return len(self.points)

def main():
    """Script principal pour indexer les FAQ"""
    logger.info("üöÄ D√©marrage de l'indexation des FAQ")
    
    # Charger les donn√©es FAQ
    faq_file = "/Users/ayoub/Documents/GitHub/FB-Ahmed/annexes/CGI/JSON/faq.json"
    with open(faq_file, "r", encoding="utf-8") as f:
        faq_data = json.load(f)
    
    logger.info(f"‚úÖ {len(faq_data)} entr√©es FAQ charg√©es")
    
    # Cr√©er le processeur
    processor = FAQProcessor()
    
    # Traiter toutes les entr√©es FAQ
    count = processor.process_all_faq_entries(faq_data)
    
    logger.info(f"‚úÖ Indexation FAQ termin√©e: {count} entr√©es trait√©es")
    
    # Sauvegarder les stats
    stats = {
        "total_faq_entries": len(faq_data),
        "indexed_faq_entries": count,
        "collection_name": processor.collection_name,
        "timestamp": datetime.now().isoformat()
    }
    
    with open("faq_indexing_stats.json", "w", encoding="utf-8") as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    
    logger.info("‚úÖ Stats FAQ sauvegard√©es dans faq_indexing_stats.json")

if __name__ == "__main__":
    main()